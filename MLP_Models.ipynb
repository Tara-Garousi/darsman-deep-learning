{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ArVaUOR5KmYF"
      ],
      "authorship_tag": "ABX9TyPiVthIan0aK4piNIhOWbHj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdiabbasidev/darsman-deep-learning/blob/main/MLP_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "twUtoBwK75qD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex1 : MLP"
      ],
      "metadata": {
        "id": "LcljIehj7ReE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(low=-10,high=10,size=(40,2),dtype=torch.float)\n",
        "print(f\"x :\\n{x}\")\n",
        "\n",
        "model1 = nn.Linear(2, 5)\n",
        "model2 = nn.Linear(5, 3)\n",
        "model3 = nn.Linear(3, 1, bias=False)\n",
        "\n",
        "mlp = nn.Sequential(model1,model2,model3)\n",
        "print(f\"\\nMLP :\\n{mlp}\")\n",
        "print(f\"bias:\\n{mlp[0].bias}\")\n",
        "print(f\"weight:\\n{mlp[0].weight}\")\n",
        "print(f\"bias:\\n{mlp[1].bias}\")\n",
        "print(f\"weight:\\n{mlp[1].weight }\")\n",
        "print(f\"bias:\\n{mlp[2].bias}\")\n",
        "print(f\"weight:\\n{mlp[2].weight }\")\n",
        "\n",
        "\n",
        "output=mlp(x)\n",
        "print(f\"\\nOutput :\\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_b2V6xL7zPm",
        "outputId": "eb830ae9-d944-464c-cb1f-b7bcf857edfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x :\n",
            "tensor([[ -4.,   7.],\n",
            "        [ -7.,  -6.],\n",
            "        [ -8.,   9.],\n",
            "        [ -9.,  -6.],\n",
            "        [ -9.,   0.],\n",
            "        [ -7.,  -9.],\n",
            "        [  9.,   1.],\n",
            "        [ -4.,   6.],\n",
            "        [  8.,  -8.],\n",
            "        [ -1.,   6.],\n",
            "        [ -1.,  -9.],\n",
            "        [  9.,  -4.],\n",
            "        [ -2.,   4.],\n",
            "        [  1.,   6.],\n",
            "        [ -2.,   8.],\n",
            "        [  3.,  -6.],\n",
            "        [  9.,  -4.],\n",
            "        [  1.,   7.],\n",
            "        [ -1.,   5.],\n",
            "        [-10.,  -4.],\n",
            "        [ -5.,   1.],\n",
            "        [  8.,   7.],\n",
            "        [ -4.,   1.],\n",
            "        [  2.,   9.],\n",
            "        [  9.,   4.],\n",
            "        [ -9.,   3.],\n",
            "        [  1.,  -7.],\n",
            "        [  6.,   9.],\n",
            "        [  4.,   5.],\n",
            "        [  6.,  -4.],\n",
            "        [  0.,   0.],\n",
            "        [  8.,  -9.],\n",
            "        [  2.,   6.],\n",
            "        [ -1.,   6.],\n",
            "        [ -9.,  -5.],\n",
            "        [ -8.,  -6.],\n",
            "        [ -8., -10.],\n",
            "        [-10.,  -4.],\n",
            "        [  1.,   2.],\n",
            "        [  9.,  -7.]])\n",
            "\n",
            "MLP :\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
            "  (1): Linear(in_features=5, out_features=3, bias=True)\n",
            "  (2): Linear(in_features=3, out_features=6, bias=False)\n",
            ")\n",
            "bias:\n",
            "Parameter containing:\n",
            "tensor([-0.6718,  0.6947,  0.5560,  0.4888,  0.4481], requires_grad=True)\n",
            "weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2512,  0.1353],\n",
            "        [-0.3340, -0.0857],\n",
            "        [-0.3166, -0.3837],\n",
            "        [ 0.3937, -0.1129],\n",
            "        [-0.6484, -0.0162]], requires_grad=True)\n",
            "bias:\n",
            "Parameter containing:\n",
            "tensor([-0.0921,  0.2107,  0.3148], requires_grad=True)\n",
            "weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1848, -0.2073,  0.2177, -0.1667,  0.3243],\n",
            "        [-0.0828, -0.4070,  0.0974, -0.4396, -0.0740],\n",
            "        [-0.1086, -0.2719, -0.1017, -0.3430,  0.1983]], requires_grad=True)\n",
            "bias:\n",
            "None\n",
            "weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.3080, -0.5040,  0.2423],\n",
            "        [-0.2057,  0.2960, -0.5643],\n",
            "        [-0.3431,  0.2819, -0.2288],\n",
            "        [ 0.2593, -0.4024, -0.3522],\n",
            "        [ 0.4175,  0.2105, -0.1686],\n",
            "        [-0.1272,  0.2233, -0.0541]], requires_grad=True)\n",
            "\n",
            "Output :\n",
            "tensor([[ 0.5269, -0.7959, -0.5547, -0.1665,  0.2096, -0.1664],\n",
            "        [ 0.9983, -0.7928, -0.9746,  0.6319,  0.7832, -0.4024],\n",
            "        [ 1.0211, -1.3782, -1.0996, -0.0934,  0.6352, -0.3409],\n",
            "        [ 1.2524, -1.0536, -1.2478,  0.7197,  1.0136, -0.4969],\n",
            "        [ 1.2107, -1.2356, -1.2432,  0.4120,  0.9083, -0.4534],\n",
            "        [ 1.0192, -0.7018, -0.9769,  0.7857,  0.8359, -0.4242],\n",
            "        [-1.0828,  1.0811,  1.2166, -0.4297, -1.1823,  0.4044],\n",
            "        [ 0.5338, -0.7656, -0.5555, -0.1152,  0.2271, -0.1736],\n",
            "        [-0.8932,  1.2237,  1.0730,  0.0757, -0.9091,  0.2919],\n",
            "        [ 0.1527, -0.3744, -0.1456, -0.2470, -0.1184, -0.0319],\n",
            "        [ 0.2570,  0.0805, -0.1573,  0.5222,  0.1449, -0.1406],\n",
            "        [-1.0481,  1.2328,  1.2128, -0.1733, -1.0945,  0.3681],\n",
            "        [ 0.2937, -0.4442, -0.2838, -0.1005,  0.0319, -0.0936],\n",
            "        [-0.1013, -0.1137,  0.1276, -0.3348, -0.3487,  0.0627],\n",
            "        [ 0.2659, -0.5655, -0.2807, -0.3056, -0.0383, -0.0646],\n",
            "        [-0.2720,  0.5111,  0.3915,  0.1927, -0.3684,  0.0701],\n",
            "        [-1.0481,  1.2328,  1.2128, -0.1733, -1.0945,  0.3681],\n",
            "        [-0.1083, -0.1440,  0.1284, -0.3861, -0.3662,  0.0699],\n",
            "        [ 0.1597, -0.3441, -0.1464, -0.1957, -0.1008, -0.0391],\n",
            "        [ 1.3655, -1.2446, -1.3829,  0.6611,  1.0936, -0.5297],\n",
            "        [ 0.6956, -0.7443, -0.6960,  0.1851,  0.4301, -0.2571],\n",
            "        [-0.9975,  0.7688,  1.0847, -0.6935, -1.1724,  0.4007],\n",
            "        [ 0.5686, -0.6140, -0.5593,  0.1412,  0.3149, -0.2099],\n",
            "        [-0.2492, -0.0743,  0.2665, -0.5326, -0.5165,  0.1317],\n",
            "        [-1.1037,  0.9901,  1.2189, -0.5836, -1.2349,  0.4262],\n",
            "        [ 1.1898, -1.3266, -1.2409,  0.2582,  0.8556, -0.4316],\n",
            "        [-0.0110,  0.2807,  0.1175,  0.3318, -0.1205, -0.0316],\n",
            "        [-0.7573,  0.4473,  0.8130, -0.7082, -0.9772,  0.3207],\n",
            "        [-0.4755,  0.3079,  0.5367, -0.4153, -0.6766,  0.1972],\n",
            "        [-0.6670,  0.8416,  0.8029, -0.0416, -0.7490,  0.2264],\n",
            "        [ 0.0674, -0.0621, -0.0137,  0.0168, -0.1282, -0.0281],\n",
            "        [-0.8863,  1.2541,  1.0723,  0.1270, -0.8916,  0.2846],\n",
            "        [-0.2284,  0.0167,  0.2642, -0.3787, -0.4639,  0.1099],\n",
            "        [ 0.1527, -0.3744, -0.1456, -0.2470, -0.1184, -0.0319],\n",
            "        [ 1.2454, -1.0839, -1.2471,  0.6684,  0.9960, -0.4897],\n",
            "        [ 1.1254, -0.9232, -1.1112,  0.6758,  0.8984, -0.4497],\n",
            "        [ 1.1532, -0.8019, -1.1143,  0.8809,  0.9686, -0.4787],\n",
            "        [ 1.3655, -1.2446, -1.3829,  0.6611,  1.0936, -0.5297],\n",
            "        [-0.0735,  0.0077,  0.1245, -0.1297, -0.2785,  0.0336],\n",
            "        [-1.0272,  1.3238,  1.2104, -0.0195, -1.0419,  0.3464]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EX2 : MLP & Loss & Optimizer"
      ],
      "metadata": {
        "id": "UAljTZGHKcSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_num=40\n",
        "feature_num=3\n",
        "target_num=5\n",
        "eta = 0.02\n",
        "N = 100\n",
        "\n",
        "x = torch.randint(low=-10,high=10,size=(sample_num,feature_num),dtype=torch.float)\n",
        "yt = torch.randn(sample_num, target_num)\n",
        "\n",
        "model1 = nn.Linear(feature_num, 7)\n",
        "model2 = nn.Linear(7, target_num, bias=False)\n",
        "mlp = nn.Sequential(model1,model2)\n",
        "# print(f\"bias:\\n{mlp[0].bias}\")\n",
        "# print(f\"weight:\\n{mlp[0].weight}\")\n",
        "# print(f\"bias:\\n{mlp[1].bias}\")\n",
        "# print(f\"weight:\\n{mlp[1].weight }\")\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(mlp.parameters(), lr=eta)\n",
        "\n",
        "loss_list=[]\n",
        "for epoch in range(N):\n",
        "    yp = mlp(x)\n",
        "    loss = loss_fn(yp, yt)\n",
        "    loss_list.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1} => Loss: {loss.item():.6f}\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "plt.plot(range(N), loss_list)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning Curve on Loss Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C85CIXdtKfPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex3 : MLP Class - Two Layer Model"
      ],
      "metadata": {
        "id": "BzlNk9ty_B7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TwoLayerModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(TwoLayerModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = TwoLayerModel(input_size=2, hidden_size=5, output_size=3)\n",
        "x1 = torch.tensor([-7., -2.])\n",
        "output1 = model(x1)\n",
        "print(f\"x1:\\n{ x1}\",)\n",
        "print(f\"Output1:\\n{ output1}\",)\n",
        "\n",
        "print(100*\"*\")\n",
        "\n",
        "x2 = torch.randint(low=-10,high=10,size=(20,2),dtype=torch.float)\n",
        "output2 = model(x2)\n",
        "print(f\"x2:\\n{ x2}\",)\n",
        "print(f\"Output2:\\n{ output2}\",)"
      ],
      "metadata": {
        "id": "weOkjcLX_DvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex4 : MLP Class - Dynamic Layer Model"
      ],
      "metadata": {
        "id": "ArVaUOR5KmYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DynamicLayerModel(nn.Module):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super(DynamicLayerModel, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x) if i == len(self.layers) - 1 else torch.relu(layer(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "layer_sizes = [2, 5, 3, 1]                # [input_size, hidden1, hidden2, ..., output_size]\n",
        "model = DynamicLayerModel(layer_sizes)\n",
        "\n",
        "x3 = torch.randint(low=-10,high=10,size=(20,2),dtype=torch.float)\n",
        "output3 = model(x3)\n",
        "print(f\"x2:\\n{ x3}\",)\n",
        "print(f\"Output2:\\n{ output3}\",)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFsXNLqvKpbL",
        "outputId": "d8209ce1-9b9b-43a0-e077-dd991579a5b2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x2:\n",
            "tensor([[  5.,   3.],\n",
            "        [ -3.,   7.],\n",
            "        [  8.,  -1.],\n",
            "        [  6.,   3.],\n",
            "        [ -4.,   9.],\n",
            "        [  8.,  -4.],\n",
            "        [  2.,   9.],\n",
            "        [  6.,  -9.],\n",
            "        [ -6.,  -2.],\n",
            "        [  2.,  -2.],\n",
            "        [  9.,  -6.],\n",
            "        [-10.,   5.],\n",
            "        [ -4.,  -5.],\n",
            "        [  1.,   2.],\n",
            "        [  0.,  -3.],\n",
            "        [ -8.,  -3.],\n",
            "        [  6.,  -7.],\n",
            "        [  2., -10.],\n",
            "        [ -4.,   8.],\n",
            "        [ -8.,  -8.]])\n",
            "Output2:\n",
            "tensor([[-0.5774],\n",
            "        [-0.2162],\n",
            "        [-0.6954],\n",
            "        [-0.6276],\n",
            "        [-0.2012],\n",
            "        [-0.3791],\n",
            "        [-0.4687],\n",
            "        [-0.2446],\n",
            "        [-0.2440],\n",
            "        [-0.2445],\n",
            "        [-0.2446],\n",
            "        [-0.2337],\n",
            "        [-0.2446],\n",
            "        [-0.3698],\n",
            "        [-0.2446],\n",
            "        [-0.2446],\n",
            "        [-0.2446],\n",
            "        [-0.2446],\n",
            "        [-0.2059],\n",
            "        [-0.2446]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}